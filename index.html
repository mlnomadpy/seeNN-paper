<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="SeeNN Framework for Multimodal Atmospheric Visibility Estimation">
  <meta property="og:title" content="SeeNN: Leveraging Multimodal Deep Learning for In-Flight Long-Range Atmospheric Visibility Estimation in Aviation Safety" />
  <meta property="og:description" content="SeeNN Framework for Multimodal Atmospheric Visibility Estimation" />
  <meta property="og:url" content="https://www.tahabouhsine.com/seeNN-paper/" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/SeeNN_Expanded.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="SeeNN: Leveraging Multimodal Deep Learning for In-Flight Long-Range Atmospheric Visibility Estimation in Aviation Safety">
  <meta name="twitter:description" content="SeeNN: Leveraging Multimodal Deep Learning for In-Flight Long-Range Atmospheric Visibility Estimation in Aviation Safety">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/SeeNN_Expanded.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="atmospheric visibility, AI, Multimodal, CV">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SeeNN: Leveraging Multimodal Deep Learning for In-Flight Long-Range Atmospheric Visibility Estimation in Aviation Safety</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SeeNN: Leveraging Multimodal Deep Learning for In-Flight Long-Range
              Atmospheric Visibility Estimation in Aviation Safety</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.tahabouhsine.com/" target="_blank">Taha Bouhsine</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.tahabouhsine.com/" target="_blank">Giuseppina Carannant</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.tahabouhsine.com/" target="_blank">Nidhal C. Bouaynaya</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.tahabouhsine.com/" target="_blank">Soufiane Idbraim</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.tahabouhsine.com/" target="_blank">Phuong Tran</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.tahabouhsine.com/" target="_blank">Grant Morfit</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.tahabouhsine.com/" target="_blank">Maggie Mayfield</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.tahabouhsine.com/" target="_blank">Charles Cliff Johnson</a><sup>3</sup>,
              </span>

            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> Electrical and Computer Engineering Department, Henery M.Rowan College of Engineering, Rowan University, Glassboro, New Jersey, USA</span>
              <span class="author-block"><sup>2</sup> IRF-SIC Laboratory, Computer Science Department, Faculty of Sciences Agadir, Ibn Zohr University, Agadir, Morocco</span>
              <span class="author-block"><sup>3</sup> William J. Hughes Technical Center, Federal Aviation Administration, Atlantic City, NJ, USA<br>American Institute of Aeronautics and Astronautics (AIAA) SciTech Forum 2025</span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span> -->

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/skywolfmo/seeNN-paper" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://www.kaggle.com/datasets/skywolfmo/seeset-v1-multimodal-in-flight-ave" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span>Dataset</span>
                  </a>
                </span>



              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <center>
          <img src="static/images/SeeNN_Expanded.png" alt="" srcset="">
        </center>
        <h2 class="subtitle has-text-centered">
          SeeNN Framework: The framework first extracts features (entropy map, surface normals map, edge
          map, depth map) from the input image. Separate encoders (ùúôùëö(¬∑) denotes modality encoders) process
          these features followed by a projection head.
          
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Deep learning (DL) models have attained state-of-the-art performance in numerous fields. Nevertheless, for certain real-world applications, existing models encounter diverse challenges, ranging from a lack of generability to new data to issues of scalability and overfitting. In this context, integrating information extracted from different modalities holds promise as a potential solution to alleviate these challenges. This paper introduces SeeNN (https://github.com/skywolfmo/seeNN-paper), a multimodal deep-learning framework for long-range atmospheric visibility estimation. Using multimodal deep learning, SeeNN fuses various modalities to estimate long-range atmospheric visibility. These modalities include RGB imagery, Edge Map, Entropy Map, Depth Map, and Normal Surface Map. Results show that in contrast to single  modality RGB, which achieves only 87.92% accuracy, multimodal deep learning models achieve an accuracy of over 96%. This significant improvement highlights the potential of multimodal approaches to enhance the accuracy and reliability of atmospheric visibility estimation, which is crucial for improving safety in applications such as aviation, maritime navigation, and autonomous vehicles. By addressing challenges such as data variability, environmental factors, and the inherent complexity of atmospheric conditions, SeeNN contributes to more reliable and robust visibility estimation systems, thereby enhancing safety and operational efficiency in critical environments.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->





  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> 
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>